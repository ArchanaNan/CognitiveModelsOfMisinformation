{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fdbf64-829e-408d-921b-cea355141c71",
   "metadata": {},
   "source": [
    "# Similarity Function for IBL\n",
    "\n",
    "The objective of this notebook is to write a similarity function for claims in Experiment 1, using LLM embeddings and Vector Search.\n",
    "This function can then be used within an IBL model that uses the speedyibl library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab1057-7d26-4476-9d5c-447dfefe05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages for openai and mongodb\n",
    "#!pip install \"pymongo[srv]\"\n",
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b43939-5190-47be-af27-5272e0038fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "#import evaluate\n",
    "import time # to calculate time\n",
    "import pandas as pd\n",
    "uri = \"mongodb+srv://archanan:hGKhjjxhr8I891i9@archcluster0.i1cmz5h.mongodb.net/?retryWrites=true&w=majority&appName=ArchCluster0\"\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7eb1e6-c4f9-4afa-a457-97adf2e63e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "788d9c97-6034-4896-bc63-ad759f98ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generate an embedding for the given text using OpenAI's API.\"\"\"\n",
    "\n",
    "    # Check for valid input\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "    try:\n",
    "        # Call OpenAI API to get the embedding\n",
    "        embedding = openai.embeddings.create(input=text, model=EMBEDDING_MODEL).data[0].embedding\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_embedding: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa4e348-89b0-458c-b19e-bbdc88a40984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['expt1_claims_collection']\n"
     ]
    }
   ],
   "source": [
    "database = client[\"expt_claims_database\"]\n",
    "collection = database[\"expt1_claims_collection\"]\n",
    "print(database.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36168954-6aa4-4043-9ee0-5a91759a6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get claims\n",
    "claims_info_df = pd.read_excel(\"DeidentifiedRawData_Exp1.xlsx\", sheet_name=\"Claims_Data\")\n",
    "#Number of tokens before generating embeddings\n",
    "#sum([num_tokens_from_string(text, \"cl100k_base\") for text in claims_info_df['text']])\n",
    "#1901\n",
    "claims_info_df[\"text_embedding_optimised\"] = claims_info_df['text'].apply(get_embedding)\n",
    "claims_subset_df = claims_info_df[[\"img_name\", \"length\", \"acc_status\", \"user_name\", \"Category\", \"text\", \"feedback\", \"text_embedding_optimised\"]]\n",
    "claims_list = claims_subset_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c388c6c-9083-4fd7-adac-373b01cdb438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "result = collection.insert_many(claims_list)\n",
    "print(result.acknowledged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f23570-0014-4bbe-916b-98b66fbadbf1",
   "metadata": {},
   "source": [
    "## Steps Further\n",
    "\n",
    "Since all the claims and their embeddings were generated and uploaded to MongoDB in the previous steps. Those cells need not be re-run again.\n",
    "In the further steps, the vector search index was created in MongoDB and similarity values and functions will be built from the similarity scores supplied by the vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c07a31-5ff3-4993-b581-f7347eb56f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get claims\n",
    "claims_info_df = pd.read_excel(\"DeidentifiedRawData_Exp1.xlsx\", sheet_name=\"Claims_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc5f9e0-3df2-4efc-a350-3d2b5f9f3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(img_name, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    img_name (str): The image name to reference against the claims database.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = collection.find_one({ \"img_name\" : img_name }, {'length': 0, 'acc_status': 0, 'user_name': 0, 'Category': 0, 'text': 0, 'feedback': 0})['text_embedding_optimised']\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"llm_cosine_vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"text_embedding_optimised\",\n",
    "                \"numCandidates\": 54,  # Number of candidate matches to consider\n",
    "                \"limit\": 54 # Return top 5 matches\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,  # Exclude the _id field\n",
    "                \"img_name\": 1,  # Exclude the text_embedding_opitimzed field\n",
    "                \"score\": {\n",
    "                    \"$meta\": \"vectorSearchScore\"  # Include the search score\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a234fcf0-ac50-42c5-8791-a239a7f94c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = claims_info_df['img_name'].to_list()\n",
    "img_dict_list = []\n",
    "for img_name in img_list:\n",
    "    img_dict_list.append(vector_search(img_name, collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06fe43c-8425-4659-8e52-d6d4a92ee5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_list)):\n",
    "    sim_dict = {}\n",
    "    for d in img_dict_list[i]:\n",
    "        sim_dict[d['img_name']] = d['score']\n",
    "    collection.update_one({\"img_name\" : img_list[i]}, {\"$set\": {'similarity_dict' : sim_dict}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ae6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = claims_info_df['img_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2979da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6514527797698975,\n",
       " 0.6307398080825806,\n",
       " 0.6276657581329346,\n",
       " 0.5906263589859009,\n",
       " 0.5894308090209961,\n",
       " 0.570685625076294,\n",
       " 0.5700032711029053,\n",
       " 0.5665214657783508,\n",
       " 0.5628194808959961,\n",
       " 0.5601605772972107,\n",
       " 0.5583720207214355,\n",
       " 0.5578873753547668,\n",
       " 0.5536611080169678,\n",
       " 0.5521847009658813,\n",
       " 0.5459815859794617,\n",
       " 0.5455030798912048,\n",
       " 0.544426441192627,\n",
       " 0.5440582036972046,\n",
       " 0.5439628958702087,\n",
       " 0.5438030958175659,\n",
       " 0.5436297059059143,\n",
       " 0.5430953502655029,\n",
       " 0.542999267578125,\n",
       " 0.5426023602485657,\n",
       " 0.5422442555427551,\n",
       " 0.5419376492500305,\n",
       " 0.5393193364143372,\n",
       " 0.5382000803947449,\n",
       " 0.5372725129127502,\n",
       " 0.5361812710762024,\n",
       " 0.535228967666626,\n",
       " 0.5338835716247559,\n",
       " 0.531522274017334,\n",
       " 0.5312227606773376,\n",
       " 0.5301764011383057,\n",
       " 0.5279795527458191,\n",
       " 0.526705265045166,\n",
       " 0.5266759395599365,\n",
       " 0.5262069702148438,\n",
       " 0.5246825814247131,\n",
       " 0.5245420932769775,\n",
       " 0.5233020782470703,\n",
       " 0.5216143131256104,\n",
       " 0.5199585556983948,\n",
       " 0.5189975500106812,\n",
       " 0.5163828730583191,\n",
       " 0.5159635543823242,\n",
       " 0.5151010155677795,\n",
       " 0.5144873857498169,\n",
       " 0.5101038217544556,\n",
       " 0.5047845244407654,\n",
       " 0.5004657506942749,\n",
       " 0.48935264348983765]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sim_dict = collection.find_one({ \"img_name\" : img_list[0] }, {'length': 0, 'acc_status': 0, 'user_name': 0, 'Category': 0, 'text': 0, 'feedback': 0, 'text_embedding_optimised': 0})['similarity_dict']\n",
    "similarities =  [sample_sim_dict[k] for k in sample_sim_dict if sample_sim_dict[k] < 1]\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0c8da62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pretest_tweet_1.jpg': 1,\n",
       " 'pretest_tweet_12.jpg': 0.9,\n",
       " 'posttest_tweet_5.jpg': 0.7849990200725184,\n",
       " 'posttest_tweet_3.jpg': 0.7679315146516624,\n",
       " 'training_tweet_11.jpg': 0.5622841907361738,\n",
       " 'training_tweet_15.jpg': 0.5556463495035463,\n",
       " 'training_tweet_10.jpg': 0.4515707704362682,\n",
       " 'training_tweet_19.jpg': 0.44778225680424505,\n",
       " 'pretest_tweet_7.jpg': 0.428450843123722,\n",
       " 'training_tweet_4.jpg': 0.40789696531350883,\n",
       " 'posttest_tweet_13.jpg': 0.3931344037646865,\n",
       " 'pretest_tweet_9.jpg': 0.38320411650436886,\n",
       " 'posttest_tweet_2.jpg': 0.3805133055031477,\n",
       " 'pretest_tweet_8.jpg': 0.3570485466737805,\n",
       " 'training_tweet_22.jpg': 0.3488513521712288,\n",
       " 'posttest_tweet_8.jpg': 0.31441089076327344,\n",
       " 'pretest_tweet_13.jpg': 0.31175416579490506,\n",
       " 'posttest_tweet_14.jpg': 0.3057765346160762,\n",
       " 'pretest_tweet_14.jpg': 0.30373203450962627,\n",
       " 'training_tweet_8.jpg': 0.30320287366951404,\n",
       " 'training_tweet_1.jpg': 0.30231564401826894,\n",
       " 'training_tweet_20.jpg': 0.30135296178944687,\n",
       " 'training_tweet_13.jpg': 0.29838615320182577,\n",
       " 'pretest_tweet_4.jpg': 0.29785269024106215,\n",
       " 'pretest_tweet_2.jpg': 0.29564901167051344,\n",
       " 'training_tweet_3.jpg': 0.2936607700648885,\n",
       " 'posttest_tweet_4.jpg': 0.2919584540163973,\n",
       " 'posttest_tweet_12.jpg': 0.27742125740323265,\n",
       " 'training_tweet_24.jpg': 0.2712070095885812,\n",
       " 'posttest_tweet_10.jpg': 0.2660570402365946,\n",
       " 'posttest_tweet_7.jpg': 0.25999833063010624,\n",
       " 'training_tweet_6.jpg': 0.2547110243496352,\n",
       " 'training_tweet_14.jpg': 0.24724121916951422,\n",
       " 'posttest_tweet_15.jpg': 0.23413100288315622,\n",
       " 'training_tweet_23.jpg': 0.23246806778524254,\n",
       " 'pretest_tweet_10.jpg': 0.22665855024417292,\n",
       " 'training_tweet_9.jpg': 0.21446137633296797,\n",
       " 'training_tweet_16.jpg': 0.207386373455695,\n",
       " 'pretest_tweet_5.jpg': 0.20722355473566043,\n",
       " 'posttest_tweet_1.jpg': 0.20461977894453906,\n",
       " 'posttest_tweet_9.jpg': 0.19615618389396342,\n",
       " 'posttest_tweet_11.jpg': 0.19537617632664347,\n",
       " 'pretest_tweet_11.jpg': 0.18849145955510188,\n",
       " 'pretest_tweet_3.jpg': 0.17912077891181352,\n",
       " 'training_tweet_7.jpg': 0.1699278089446604,\n",
       " 'training_tweet_25.jpg': 0.16459218653995145,\n",
       " 'training_tweet_18.jpg': 0.15007517680061214,\n",
       " 'training_tweet_21.jpg': 0.147747067663533,\n",
       " 'posttest_tweet_6.jpg': 0.14295814551389935,\n",
       " 'pretest_tweet_15.jpg': 0.13955119689041248,\n",
       " 'training_tweet_2.jpg': 0.1152131075688275,\n",
       " 'training_tweet_5.jpg': 0.08567971116223812,\n",
       " 'pretest_tweet_6.jpg': 0.06170134531357496,\n",
       " 'training_tweet_12.jpg': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnge = max(similarities) - min(similarities)\n",
    "min_sample_sim = min(similarities)\n",
    "sample_scaled_sim = {}\n",
    "for k in sample_sim_dict:\n",
    "    if sample_sim_dict[k] < 1:\n",
    "        sample_scaled_sim[k] = (sample_sim_dict[k] - min_sample_sim)*0.9/rnge\n",
    "    else:\n",
    "        sample_scaled_sim[k] = 1\n",
    "sample_scaled_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "745f1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_list)):\n",
    "    scaled_sim_dict = {}\n",
    "    sim_dict = collection.find_one({ \"img_name\" : img_list[i] }, {'length': 0, 'acc_status': 0, 'user_name': 0, 'Category': 0, 'text': 0, 'feedback': 0, 'text_embedding_optimised': 0})['similarity_dict']\n",
    "    similarities =  [sim_dict[k] for k in sim_dict if sim_dict[k] < 1]\n",
    "    max_similarity = max(similarities)\n",
    "    min_similarity = min(similarities)\n",
    "    range_similarity = max_similarity - min_similarity\n",
    "    for k in sim_dict:\n",
    "        if sim_dict[k] < 1:\n",
    "            scaled_sim_dict[k] = (sim_dict[k] - min_similarity)*0.9/range_similarity\n",
    "        else:\n",
    "            scaled_sim_dict[k] = sim_dict[k]\n",
    "    collection.update_one({\"img_name\" : img_list[i]}, {\"$set\": {'scaled_similarity_dict' : scaled_sim_dict}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706c0bea-97f9-4373-8be5-5604bea780f8",
   "metadata": {},
   "source": [
    "Now the collection `Expt1_Claims` in MongoDB contains a similarity dictionary for all the claims in Experiment 1 that can be used in the IBL cognitive model similarity function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
